{"cells":[{"cell_type":"markdown","metadata":{},"source":[" this file is to generate predictions based on input data"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import keras\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","from sklearn import preprocessing\n","from keras.models import load_model\n","from datafunctions import convert_data, normalise, gen_sequence\n","\n","np.random.seed(1010)\n","PYTHONHASHSEED = 0\n","\n","model_path = \"./finfanOUT/bin_model.h5\"\n","\n","# load the model\n","if os.path.isfile(model_path):\n","    estimator = load_model(model_path)\n"]},{"cell_type":"markdown","metadata":{},"source":[" Set up the data to generate predictions on"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# constant definitions take these from what the initial model was trained with\n","sequence_length = 50\n","minmax_scaler = preprocessing.MinMaxScaler()\n","# below should be all input variables given to the model\n","# list of all sensors used\n","sensor_cols = [\"speed\"]\n","# list of other variables\n","sequence_cols = [\"status\"]\n","sequence_cols.extend(sensor_cols)\n","# read in data to predict on\n","test_df2 = pd.read_csv(\n","    \"./finfanIn/Data_Extract_Train2.txt\", sep=\"\\t\", header=None, low_memory=False\n",")\n","test_dfs = [test_df2]"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["                      time   speed  status  id\n0      2018-02-08 11:37:30  2086.0     0.0   0\n1      2018-02-08 11:38:30  2085.0     0.0   0\n2      2018-02-08 11:39:30  2086.0     0.0   0\n3      2018-02-08 11:40:30  2085.0     0.0   0\n4      2018-02-08 11:42:30  2086.0     0.0   0\n...                    ...     ...     ...  ..\n289615 2018-07-29 07:12:58    -3.0     1.0   0\n289616 2018-07-29 15:04:29    -3.0     1.0   0\n289617 2018-07-30 06:47:29    -3.0     1.0   0\n289618 2018-07-30 09:36:29    -3.0     1.0   0\n289619 2018-07-30 09:38:59    -3.0     1.0   0\n\n[289620 rows x 4 columns]\n                       time     speed  status  id  time_norm\n0       1518089850000000000  0.993343     0.0   0   0.182789\n1       1518089910000000000  0.992867     0.0   0   0.182793\n2       1518089970000000000  0.993343     0.0   0   0.182796\n3       1518090030000000000  0.992867     0.0   0   0.182799\n4       1518090150000000000  0.993343     0.0   0   0.182806\n...                     ...       ...     ...  ..        ...\n289615  1532848378000000000  0.000000     1.0   0   0.994764\n289616  1532876669000000000  0.000000     1.0   0   0.996321\n289617  1532933249000000000  0.000000     1.0   0   0.999434\n289618  1532943389000000000  0.000000     1.0   0   0.999992\n289619  1532943539000000000  0.000000     1.0   0   1.000000\n\n[289620 rows x 5 columns]\n                       time     speed  status  id  time_norm\n0       1518089850000000000  0.993343     0.0   0   0.182789\n1       1518089910000000000  0.992867     0.0   0   0.182793\n2       1518089970000000000  0.993343     0.0   0   0.182796\n3       1518090030000000000  0.992867     0.0   0   0.182799\n4       1518090150000000000  0.993343     0.0   0   0.182806\n...                     ...       ...     ...  ..        ...\n289615  1532848378000000000  0.000000     1.0   0   0.994764\n289616  1532876669000000000  0.000000     1.0   0   0.996321\n289617  1532933249000000000  0.000000     1.0   0   0.999434\n289618  1532943389000000000  0.000000     1.0   0   0.999992\n289619  1532943539000000000  0.000000     1.0   0   1.000000\n\n[289620 rows x 5 columns]\n"]}],"source":["# converts the list of training dataframes to the correct format and adds it a single dataframe with an id column\n","# it cleans the data and merges all the time columns\n","# and normalises the data using the scaler chosen, by default uses minmax which scales it to between 0 and 1\n","test_df = None\n","for i, t in enumerate(test_dfs):\n","    if test_df is None:\n","        test_df = normalise(convert_data(t, i), minmax_scaler)\n","    else:\n","        test_df = pd.concat(\n","            [test_df, normalise(convert_data(t, i), minmax_scaler)],\n","            copy=False,\n","        )\n","print(test_df)\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# generates the data chunks to be input into the model from the data previously formatted above\n","seq_gen = (\n","    list(gen_sequence(test_df[test_df[\"id\"] == id], sequence_length, sequence_cols))\n","    for id in test_df[\"id\"].unique()\n",")\n","\n","seq_array = np.concatenate(list(seq_gen)).astype(np.float32)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["1448/1448 [==============================] - 13s 8ms/step\n","[[0.14063083]\n"," [0.1406382 ]\n"," [0.14063159]\n"," ...\n"," [0.02087164]\n"," [0.02087165]\n"," [0.02087165]]\n"]}],"source":["# generate predictions for the input data comes out as a number between 0 and 1 for the confidence that it is within window until failure\n","predictions = estimator.predict(seq_array, verbose=1, batch_size=200)\n","print(predictions)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8-final"},"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit","metadata":{"interpreter":{"hash":"2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"}}}}}